---
title: "Predicting Fall Risk Through Foot Tapping  Regression Analysis"
author: "Logan Marshall"
format: pdf
editor: visual
---

```{r, echo=FALSE, message=FALSE}
library(readxl)

#excel_sheets("DataAllFeatures.xlsx")
#df <- read_excel("DataAllFeatures.xlsx", sheet = "Sheet1")
#dom_df <- read_excel("~/DataAllFeatures.xlsx", sheet = "Dominant foot")
ndom_df <- read_excel("~/DataAllFeatures.xlsx", sheet = "Nondominant")
```

##Data Cleaning

```{r, echo=FALSE,message=FALSE}
library(dplyr)
ndom_df <- ndom_df %>% select(-(ParticipantName:acctime_tkeoMAE))
ndom_df <- ndom_df %>% select (-acctime_tapsforce)
ndom_df <- ndom_df %>% select (-(acctime_Rsquaretinsfreqfitted:acctime_MSEexpfitamptap))
ndom_df <- ndom_df %>% select (-acctime_MSElinearfitamptap)
ndom_df <- ndom_df %>% select (-(gyrotime_meandegreewithbumps:gyrotime_cvdegreewithbumps))
ndom_df <- ndom_df %>% select (-gyrotime_numberbumps)
ndom_df <- ndom_df %>% select (-gyrotimefrequency_numberinteruption)
ndom_df <- ndom_df %>% select (-gyrotimefrequency_numberfreez)
ndom_df <- ndom_df %>% select(-(gyrotimefrequency_maxdominantfrequencytime:gyrotimefrequency_interceptdominantfrequency))
ndom_df <- ndom_df %>% select(-(gyrotimefrequency_longestinteruptcsa30:gyrotimefrequency_csavariability))
ndom_df <- ndom_df %>% select (-(acctime_Power_1:acctime_Power_4))
ndom_df <- ndom_df %>% select (-(gyrotime_Power_1:gyrotime_Power_3))
ndom_df <- ndom_df %>% select (-gyrotimefrequency_totalTimecsainteruption)
```

## Lasso

### Gait Lasso

```{r, message=FALSE}
library(caret)
set.seed(101)

gait_train_index <- createDataPartition(ndom_df$`Gait Time`, p = 0.7, list = FALSE)
#gait_train_index <- createDataPartition(ndom_df$Sex, p = 0.7, list = FALSE)
gait_train_df <- ndom_df[gait_train_index, ]
gait_test_df  <- ndom_df[-gait_train_index, ]

names(gait_train_df) <- make.names(names(gait_train_df), unique = TRUE)
names(gait_test_df) <- make.names(names(gait_test_df), unique = TRUE)

#prop.table(table(ndom_df$Sex))
#prop.table(table(gait_train_df$Sex))
#prop.table(table(gait_test_df$Sex))
```

```{r}
y <- gait_train_df$Gait.Time

x <- model.matrix(Gait.Time ~ . - Gait.Time - TUG.Time, data=gait_train_df)

library(glmnet)
whole_lasso_fit <- glmnet(x,y,alpha=1)

#plot(whole_lasso_fit, xvar="lambda")
set.seed(101)
whole_cv_lasso <- cv.glmnet(x, y, alpha = 1)

#plot(whole_cv_lasso)

#best lambdas
best_lambda <- whole_cv_lasso$lambda.min  #lambda that minimizes CV error/test MSE
best_lambda_1se <- whole_cv_lasso$lambda.1se  #1-SE rule (simpler model)
#best_lambda
#best_lambda_1se

#coef(whole_cv_lasso, s = "lambda.min") #coefficients at best lambda

lasmin <- glmnet(x,y,alpha = 1, lambda=best_lambda)

gait_coef <- coef(lasmin)

#coef(lasmin)
Gait_Las_columns <- rownames(coef(lasmin))[which(coef(lasmin) != 0)]
Gait_Las_columns <- Gait_Las_columns[Gait_Las_columns != "(Intercept)"]

x_test <- model.matrix( ~ . - Gait.Time - TUG.Time, data = gait_test_df)
y_test <- gait_test_df$Gait.Time

y_pred <- predict(lasmin, s = best_lambda, newx = x_test)

rmse_test <- sqrt(mean((y_test - y_pred)^2))
rmse_test

y_train_pred <- predict(lasmin, s = best_lambda, newx = x)

rmse_train <- sqrt(mean((y - y_train_pred)^2))
sst_train <- sum((y - mean(y))^2)
sse_train <- sum((y - y_train_pred)^2)
rsq_train <- 1 - sse_train/sst_train

rsq_train
rmse_train

sst_test <- sum((y_test - mean(y_test))^2)
sse_test <- sum((y_test - y_pred)^2)
rsq_test <- 1 - sse_test/sst_test
rsq_test
```

```{r}
gait_lasreg <- lm(Gait.Time ~ acctime_ITIsd + Age.numerical + acctime_Powerlog_3  + accTimeFrequency_SDSumenergy_2 + accTimeFrequency_MeanSumenergy_3, data = gait_train_df)
summary(gait_lasreg)
#plot(gait_lasreg)
plot(gait_lasreg, which = 3)
#right-skewed, but mostly normal distribution
#Increase in Scale-Location plot but sqrt(std. res) mainly stays below 1, therefore would not claim that assumption of equal variance fails but interesting to observe

pred_train <- predict(gait_lasreg, newdata = gait_train_df)
rmse_train <- sqrt(mean((gait_train_df$Gait.Time - pred_train)^2))

pred_test <- predict(gait_lasreg, newdata = gait_test_df)
rmse_test <- sqrt(mean((gait_test_df$Gait.Time - pred_test)^2))

rmse_train
rmse_test

#library(broom)
#library(knitr)
#library(kableExtra)

#tidy_gait <- tidy(gait_lasreg)

#mod_sum <- summary(gait_lasreg)
#model_stats <- tibble(
#  Metric = c("R-squared", "Adjusted R-squared", "F-statistic", "Model p-value"),
#  Value = c(
#    round(mod_sum$r.squared, 4),
#    round(mod_sum$adj.r.squared, 4),
#    round(mod_sum$fstatistic[1], 3),
#    round(pf(mod_sum$fstatistic[1], mod_sum$fstatistic[2],
#             mod_sum$fstatistic[3], lower.tail = FALSE), 4)
#  )
#)

# First table: coefficients
#kable(tidy_gait,
#      caption = "Summary of Nonzero Coefficients for Gait Time Model",
#      digits = 4,
#      col.names = c("Term", "Estimate", "Std. Error", "t value", "p value")) %>%
#  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))

# Second table: model-level statistics
#kable(model_stats,
#      caption = "Model-Level Statistics",
#      digits = 4,
#      col.names = c("Metric", "Value")) %>%
#  kable_styling(full_width = FALSE, position = "center", bootstrap_options = "hover")
```

### Tug Lasso

```{r}
set.seed(101)
tug_train_index <- createDataPartition(ndom_df$`TUG Time`, p = 0.7, list = FALSE)
tug_train_df <- ndom_df[tug_train_index, ]
tug_test_df  <- ndom_df[-tug_train_index, ]

names(tug_train_df) <- make.names(names(tug_train_df), unique = TRUE)
names(tug_test_df) <- make.names(names(tug_test_df), unique = TRUE)
```

```{r}
y <- tug_train_df$TUG.Time

x <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time, data=tug_train_df)

whole_lasso_fit <- glmnet(x,y,alpha=1)

#plot(whole_lasso_fit, xvar="lambda")

set.seed(101)
whole_cv_lasso <- cv.glmnet(x, y, alpha = 1)

#plot(whole_cv_lasso)

#best lambdas
best_lambda <- whole_cv_lasso$lambda.min       #λ that minimizes CV error
best_lambda_1se <- whole_cv_lasso$lambda.1se   #1-SE rule (simpler model)
#best_lambda
#best_lambda_1se

#coef(whole_cv_lasso, s = "lambda.min")   #coefficients at best λ
lasmin <- glmnet(x,y,alpha = 1, lambda=best_lambda)

tug_coef <- coef(lasmin)
#coef(lasmin)
Tug_Las_columns <- rownames(tug_coef)[which(tug_coef != 0)]
Tug_Las_columns <- Tug_Las_columns[Tug_Las_columns != "(Intercept)"]

x_test <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time,, data = tug_test_df)
y_test <- tug_test_df$TUG.Time

y_pred <- predict(lasmin, s = best_lambda, newx = x_test)

rmse_test <- sqrt(mean((y_test - y_pred)^2))
rmse_test

y_train_pred <- predict(lasmin, s = best_lambda, newx = x)

rmse_train <- sqrt(mean((y - y_train_pred)^2))
sst_train <- sum((y - mean(y))^2)
sse_train <- sum((y - y_train_pred)^2)
rsq_train <- 1 - sse_train/sst_train

rmse_train
rsq_train
```

```{r}
tug_lasreg <- lm(
  TUG.Time ~ acctime_ITImean +
    acctime_ITIsd + acctime_Meansignal_1 + acctime_Kurtosissignal_2 + acctime_Minsignal_1 +
    acctime_Powerlog_2 + acctime_Powerlog_4 + accTimeFrequency_SDSumenergy_4 +
    accTimeFrequency_SlopeInsfreqHHT_1 + accTimeFrequency_SlopeInsfreqHHT_4 +
    accfrequency_fwhmaccpsd + gyrotime_Meansignal_1 + gyrotime_Meansignal_3 +
    gyrotime_Kurtosissignal_2 + gyrotime_Kurtosissignal_3 + gyrotime_Skewsignal_2 +
    gyrotime_Maxsignal_2 + gyrotime_Minsignal_3 + gyrotime_Rangesignal_3 +
    gyrotime_fitlineliftupslope + gyrotime_cvdropdown + gyrotime_sddegreewithoutbumps +
    gyrotimefrequency_mincsat + Age.numerical,
  data = tug_train_df
)
summary(tug_lasreg)

pred_train <- predict(tug_lasreg, newdata = tug_train_df)
rmse_train <- sqrt(mean((tug_train_df$TUG.Time - pred_train)^2))

pred_test <- predict(tug_lasreg, newdata = tug_test_df)
rmse_test <- sqrt(mean((tug_test_df$TUG.Time - pred_test)^2))

rmse_train
rmse_test
```

##Stepwise

##Gait Step

###Backward

```{r}
#full_gait <- lm(`Gait Time` ~ ., data = reduced_train)
#backward_gait <- step(full_gait)

#summary(backward_gait)
#vif(backward_gait)
#plot(backward_gait)
```

### Forward Selection

```{r}
corr_matrix <- cor(gait_train_df[ , !(names(gait_train_df) %in% c("TUG.Time", "Gait.Time"))])
high_corr <- findCorrelation(corr_matrix, cutoff = 0.8)

predictor_names <- names(gait_train_df)[!(names(gait_train_df) %in% c("TUG.Time", "Gait.Time"))]
reduced_predictors <- predictor_names[-high_corr]

reduced_train <- gait_train_df[, c("Gait.Time", reduced_predictors)]

full_gait <- lm(Gait.Time ~ ., data = reduced_train)
model.empty <- lm(Gait.Time~1, data=reduced_train) #Intercept only
forward_gait <- step(model.empty,direction = "forward",
                       scope = list(lower = model.empty, upper = full_gait)) #AIC
summary(forward_gait)
#vif(forward_gait)
#plot(forward_gait)

#Predict on test set

test_preds <- predict(forward_gait, newdata = gait_test_df)

y_true <- gait_test_df$Gait.Time

rmse_test <- sqrt(mean((y_true - test_preds)^2))
rmse_test

#Training predictions
train_preds <- predict(forward_gait, newdata = reduced_train)

#True training values
y_train <- reduced_train$Gait.Time

#Training RMSE
rmse_train <- sqrt(mean((y_train - train_preds)^2))
rmse_train

sse <- sum((y_true - test_preds)^2)
sst <- sum((y_true - mean(y_true))^2)
test_r2 <- 1 - sse/sst
test_r2
```

### Mixed

```{r}
mixed_gait <- step(model.empty,direction = "both", scope = list(lower = model.empty, upper = full_gait))
summary(mixed_gait)
#vif(mixed_gait)
#plot(mixed_gait)

test_preds <- predict(mixed_gait, newdata = gait_test_df)

y_true <- gait_test_df$Gait.Time

rmse_test <- sqrt(mean((y_true - test_preds)^2))
rmse_test

train_preds <- predict(mixed_gait, newdata = reduced_train)
y_train <- reduced_train$Gait.Time
rmse_train <- sqrt(mean((y_train - train_preds)^2))
rmse_train

sse <- sum((y_true - test_preds)^2)
sst <- sum((y_true - mean(y_true))^2)
test_r2 <- 1 - sse/sst
test_r2
```

##Tug Step

### Backward

```{r}

#backward_tug <- step(full_tug)

#summary(backward_tug)
#vif(backward_tug)
#plot(backward_tug)
#AIC(backward_tug)
```

### Forward Selection

```{r}
corr_matrix <- cor(tug_train_df[ , !(names(tug_train_df) %in% c("TUG.Time", "Gait.Time"))])
high_corr <- findCorrelation(corr_matrix, cutoff = 0.75)

predictor_names <- names(tug_train_df)[!(names(tug_train_df) %in% c("TUG.Time", "Gait.Time"))]
reduced_predictors <- predictor_names[-high_corr]

reduced_train <- tug_train_df[, c("TUG.Time", reduced_predictors)]

full_tug <- lm(TUG.Time ~ ., data = reduced_train)

model.empty <- lm(TUG.Time~1, data=reduced_train) #Intercept only
forward_tug <- step(model.empty,direction = "forward",
                       scope = list(lower = model.empty, upper = full_tug)) #AIC
summary(forward_tug)
#vif(forward_tug)
#plot(forward_tug)

test_preds <- predict(forward_tug, newdata = tug_test_df)

y_true <- tug_test_df$TUG.Time

rmse_test <- sqrt(mean((y_true - test_preds)^2))
rmse_test

train_preds <- predict(forward_tug, newdata = reduced_train)
y_train <- reduced_train$TUG.Time
rmse_train <- sqrt(mean((y_train - train_preds)^2))
rmse_train
```

### Mixed

```{r}
mixed_tug <- step(model.empty,direction = "both", scope = list(lower = model.empty, upper = full_tug))
summary(mixed_tug)
#vif(mixed_tug)
#plot(mixed_tug)

test_preds <- predict(mixed_tug, newdata = tug_test_df)

y_true <- tug_test_df$TUG.Time

rmse_test <- sqrt(mean((y_true - test_preds)^2))
rmse_test
train_preds <- predict(mixed_tug, newdata = reduced_train)
y_train <- reduced_train$TUG.Time
rmse_train <- sqrt(mean((y_train - train_preds)^2))
rmse_train
```

##Ridge

### Gait Ridge

```{r}
y <- gait_train_df$Gait.Time

x <- model.matrix( ~ . - Gait.Time - TUG.Time, data=gait_train_df)

whole_lasso_fit <- glmnet(x,y,alpha=0)

#plot(whole_lasso_fit, xvar="lambda")
set.seed(101)
whole_cv_lasso <- cv.glmnet(x, y, alpha = 0)

#plot(whole_cv_lasso)

#best lambdas
best_lambda <- whole_cv_lasso$lambda.min  #lambda that minimizes CV error/test MSE
best_lambda_1se <- whole_cv_lasso$lambda.1se  #1-SE rule (simpler model)
#best_lambda
#best_lambda_1se

lasmin <- glmnet(x,y,alpha = 0, lambda=best_lambda)

#coef(lasmin)

x_test <- model.matrix( ~ . - Gait.Time - TUG.Time, data = gait_test_df)

y_test <- gait_test_df$Gait.Time

y_pred <- predict(lasmin, s = best_lambda, newx = x_test)

rmse_test <- sqrt(mean((y_test - y_pred)^2))
rmse_test

y_train_pred <- predict(lasmin, s = best_lambda, newx = x)

rmse_train <- sqrt(mean((y - y_train_pred)^2))
sst_train <- sum((y - mean(y))^2)
sse_train <- sum((y - y_train_pred)^2)
rsq_train <- 1 - sse_train/sst_train

rmse_train
rsq_train
```

### Tug Ridge

```{r}
y <- tug_train_df$TUG.Time

x <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time, data=tug_train_df)

whole_lasso_fit <- glmnet(x,y,alpha=0)

#plot(whole_lasso_fit, xvar="lambda")
set.seed(101)
whole_cv_lasso <- cv.glmnet(x, y, alpha = 0)

#plot(whole_cv_lasso)

#best lambdas
best_lambda <- whole_cv_lasso$lambda.min       #λ that minimizes CV error
best_lambda_1se <- whole_cv_lasso$lambda.1se   #1-SE rule (simpler model)
#best_lambda
#best_lambda_1se

#coef(whole_cv_lasso, s = "lambda.min")   #coefficients at best λ
lasmin <- glmnet(x,y,alpha = 0, lambda=best_lambda)

Tug_ridge_columns <- rownames(coef(lasmin))[which(coef(lasmin) != 0)]
Tug_ridge_columns <- Tug_ridge_columns[Tug_ridge_columns != "(Intercept)"]

#coef(lasmin)

x_test <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time, data = tug_test_df)
y_test <- tug_test_df$TUG.Time

y_pred <- predict(lasmin, s = best_lambda, newx = x_test)

rmse_test <- sqrt(mean((y_test - y_pred)^2))
rmse_test

y_train_pred <- predict(lasmin, s = best_lambda, newx = x)

rmse_train <- sqrt(mean((y - y_train_pred)^2))
sst_train <- sum((y - mean(y))^2)
sse_train <- sum((y - y_train_pred)^2)
rsq_train <- 1 - sse_train/sst_train

rmse_train
rsq_train
```

## Random Forest

### Gait Forest

```{r, fig.width=10, fig.height=7}
library(randomForest)
set.seed(101)

gaitRF <- randomForest(
  Gait.Time ~ .,
  data = subset(gait_train_df, select = -TUG.Time), #remove the other response variable
  mtry = (ncol(gait_train_df) - 2)/3, #total predictors minus response variables
  importance = TRUE
)
gaitRF


gaitRF_pred <- predict(gaitRF, newdata = subset(gait_test_df, select = -c(Gait.Time, TUG.Time)))

gaitRF_true <- gait_test_df$Gait.Time

rmse_test <- sqrt(mean((gaitRF_pred - gaitRF_true)^2))

rmse_test

varImpPlot(gaitRF, n.var = 10, main = "Top 10 Variable Importances")

importance_vals <- importance(gaitRF)

#Convert to data frame for easier sorting
importance_df <- data.frame(
  Feature = rownames(importance_vals),
  importance_vals
)

top20 <- importance_df[order(-importance_df$X.IncMSE), ][1:20, ]
print(top20)
```

### Gait Bagging

```{r, fig.width=10, fig.height=8}
set.seed(101)
gaitBag <- randomForest(
  Gait.Time ~ .,
  data = subset(gait_train_df, select = -TUG.Time), #remove the other response variable
  mtry = (ncol(gait_train_df) - 2), #total predictors minus response and excluded var
  importance = TRUE
)
gaitBag

gaitBag_pred <- predict(gaitBag, newdata = subset(gait_test_df, select = -c(Gait.Time, TUG.Time)))

gaitBag_true <- gait_test_df$Gait.Time

rmse_test <- sqrt(mean((gaitBag_pred - gaitBag_true)^2))

rmse_test

varImpPlot(gaitBag, n.var = 10, main = "Top 10 Variable Importances")
```

### Gait Boosting

```{r}
library(gbm)
set.seed(101)
gaitgbm <- gbm(Gait.Time ~ .,
  data = subset(gait_train_df, select = -TUG.Time), #remove the other response variable
  distribution = "gaussian",
  n.trees = 5000, 
  interaction.depth = 1,
  shrinkage = 0.01,
  cv.folds = 5)

#summary(gaitgbm)

set.seed(101)
best_iter <- gbm.perf(gaitgbm, method = "cv")
best_iter
#118

train_pred <- predict(gaitgbm, newdata = subset(gait_train_df, select = -c(Gait.Time, TUG.Time)), n.trees = best_iter)
test_pred  <- predict(gaitgbm, newdata = subset(gait_test_df, select = -c(Gait.Time, TUG.Time)), n.trees = best_iter)

rmse_train <- sqrt(mean((train_pred - gait_train_df$Gait.Time)^2))
rmse_test  <- sqrt(mean((test_pred  - gait_test_df$Gait.Time)^2))

r2_train <- 1 - sum((gait_train_df$Gait.Time - train_pred)^2) / sum((gait_train_df$Gait.Time - mean(gait_train_df$Gait.Time))^2)
r2_test  <- 1 - sum((gait_test_df$Gait.Time - test_pred)^2) / sum((gait_test_df$Gait.Time - mean(gait_test_df$Gait.Time))^2)

rmse_train; rmse_test
r2_train
```

### Tug Forest

```{r, fig.width=10, fig.height=8}
set.seed(101)

tugRF <- randomForest(
  TUG.Time ~ .,
  data = subset(tug_train_df, select = -Gait.Time), #remove the other response variable
  mtry = (ncol(tug_train_df) - 2)/3, #total predictors minus response and excluded var
  importance = TRUE
)
tugRF


tugRF_pred <- predict(tugRF, newdata = subset(tug_test_df, select = -c(Gait.Time, TUG.Time)))

tugRF_true <- tug_test_df$TUG.Time

rmse_test <- sqrt(mean((tugRF_pred - tugRF_true)^2))

rmse_test

varImpPlot(tugRF, n.var = 10, main = "Top 10 Variable Importances")
```

### Tug Bagging

```{r}
set.seed(101)
tugBag <- randomForest(
  TUG.Time ~ .,
  data = subset(tug_train_df, select = -Gait.Time), #remove the other response variable
  mtry = (ncol(tug_train_df) - 2), #total predictors minus response and excluded var
  importance = TRUE
)
tugBag

tugBag_pred <- predict(tugBag, newdata = subset(tug_test_df, select = -c(Gait.Time, TUG.Time)))

tugBag_true <- tug_test_df$TUG.Time

rmse_test <- sqrt(mean((tugBag_pred - tugBag_true)^2))

rmse_test
```

### Tug Boosting

```{r}
set.seed(101)
tuggbm <- gbm(TUG.Time ~ .,
  data = subset(tug_train_df, select = -Gait.Time), #remove the other response variable
  distribution = "gaussian",
  n.trees = 5000, 
  interaction.depth = 1,
  shrinkage = 0.01,
  cv.folds = 5)

#summary(tuggbm)

best_iter <- gbm.perf(tuggbm, method = "cv")
best_iter
#419

train_pred <- predict(tuggbm, newdata = subset(tug_train_df, select = -c(Gait.Time, TUG.Time)), n.trees = best_iter)
test_pred  <- predict(tuggbm, newdata = subset(tug_test_df, select = -c(Gait.Time, TUG.Time)), n.trees = best_iter)

rmse_train <- sqrt(mean((train_pred - tug_train_df$TUG.Time)^2))
rmse_test  <- sqrt(mean((test_pred  - tug_test_df$TUG.Time)^2))

r2_train <- 1 - sum((tug_train_df$TUG.Time - train_pred)^2) / sum((tug_train_df$TUG.Time - mean(tug_train_df$TUG.Time))^2)
r2_test  <- 1 - sum((tug_test_df$TUG.Time - test_pred)^2) / sum((tug_test_df$TUG.Time - mean(tug_test_df$TUG.Time))^2)

rmse_train; rmse_test
r2_train
```

## Support Vector Machines

### Gait SVM

```{r}
library(e1071)
library(kernlab)

x_train <- gait_train_df[, !(names(gait_train_df) %in% c("Gait.Time", "TUG.Time"))]
y_train <- gait_train_df$Gait.Time

x_test <- gait_test_df[, !(names(gait_test_df) %in% c("Gait.Time", "TUG.Time"))]
y_test <- gait_test_df$Gait.Time

set.seed(101)
#Define train control for 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

#Tuning grid for radial kernels
svm_grid <- expand.grid(
  sigma = c(0.001, 0.01, 0.1),      #used only for RBF (sigma = gamma)
  C = c(0.01, 0.1, 1, 10)         #cost parameter
)

x_train <- as.data.frame(x_train)
x_test  <- as.data.frame(x_test)

#Training SVM with radial basis function kernel
svm_tuned <- train(
  x = x_train,
  y = y_train,
  method = "svmRadial",           
  tuneGrid = svm_grid,
  trControl = train_control,
  preProcess = c("center", "scale")
)
#print(svm_tuned)
#svm_tuned$finalModel
#sigma = 0.001, C=1

y_train_pred <- predict(svm_tuned, newdata = x_train)
y_test_pred  <- predict(svm_tuned, newdata = x_test)

rmse_train <- sqrt(mean((y_train - y_train_pred)^2))
rmse_test  <- sqrt(mean((y_test - y_test_pred)^2))

rsq_train <- 1 - sum((y_train - y_train_pred)^2) / sum((y_train - mean(y_train))^2)
rsq_test  <- 1 - sum((y_test - y_test_pred)^2) / sum((y_test - mean(y_test))^2)

rmse_train; rmse_test
rsq_train; rsq_test
```

### Tug SVM

```{r}
x_train <- tug_train_df[, !(names(tug_train_df) %in% c("Gait.Time", "TUG.Time"))]
y_train <- tug_train_df$TUG.Time

x_test <- tug_test_df[, !(names(tug_test_df) %in% c("Gait.Time", "TUG.Time"))]
y_test <- tug_test_df$TUG.Time

set.seed(101)
train_control <- trainControl(method = "cv", number = 10)

svm_grid <- expand.grid(
  sigma = c(0.001, 0.01, 0.1),    
  C = c(0.01, 0.1, 1, 10)         
)

x_train <- as.data.frame(x_train)
x_test  <- as.data.frame(x_test)

svm_tuned <- train(
  x = x_train,
  y = y_train,
  method = "svmRadial",           
  tuneGrid = svm_grid,
  trControl = train_control,
  preProcess = c("center", "scale")
)
#print(svm_tuned)
#svm_tuned$finalModel

y_train_pred <- predict(svm_tuned, newdata = x_train)
y_test_pred  <- predict(svm_tuned, newdata = x_test)

rmse_train <- sqrt(mean((y_train - y_train_pred)^2))
rmse_test  <- sqrt(mean((y_test - y_test_pred)^2))

rsq_train <- 1 - sum((y_train - y_train_pred)^2) / sum((y_train - mean(y_train))^2)
rsq_test  <- 1 - sum((y_test - y_test_pred)^2) / sum((y_test - mean(y_test))^2)

rmse_train; rmse_test
rsq_train; rsq_test
```

## PCR

### Gait PCR
```{r}
results <- prcomp(gait_train_df[ , !(names(gait_train_df) %in% c("TUG.Time", "Gait.Time"))], scale. = TRUE) #12 components for both GAIT and TUG training sets
summary(results)
plot(results, type="lines")
#Keep number of components such that 80% of the variance from original data is retained.
#Scree plot says 2-3 components 
#Kaiser criterion says 22 gait
#12 if we want to retain 80% of variance

#head(results$rotation[, 1:3])

scr <- results$x[,1:12]
train_df <- data.frame(Gait.Time = gait_train_df$Gait.Time, scr)
pcregmod <- lm(Gait.Time ~ ., data=train_df)
summary(pcregmod)

train_pred <- predict(pcregmod)
train_rmse <- sqrt(mean((train_pred - train_df$Gait.Time)^2))
train_rmse

test_predictors <- gait_test_df[, !(names(gait_test_df) %in% c("TUG.Time", "Gait.Time"))]
test_pca_scores <- predict(results, newdata = test_predictors)[, 1:12]

test_df <- data.frame(Gait.Time = gait_test_df$Gait.Time, test_pca_scores)
pcr_pred <- predict(pcregmod, newdata = test_df)


y_test <- test_df$Gait.Time
test_rmse <- sqrt(mean((pcr_pred - y_test)^2))
test_rmse
```
### TUG PCR
```{r}
results <- prcomp(tug_train_df[ , !(names(tug_train_df) %in% c("TUG.Time", "Gait.Time"))], scale. = TRUE) #12 components for both GAIT and TUG training sets
summary(results)
plot(results, type="lines")
#Keep number of components such that 80% of the variance from original data is retained.
#Scree plot says 3 components 
#Kaiser criterion says 22 tug
#12 if we want to retain 80% of variance

#head(results$rotation[, 1:3])

scr <- results$x[,1:12]
train_df <- data.frame(TUG.Time = tug_train_df$TUG.Time, scr)
pcregmod <- lm(TUG.Time ~ ., data=train_df)
summary(pcregmod)

train_pred <- predict(pcregmod)
train_rmse <- sqrt(mean((train_pred - train_df$TUG.Time)^2))
train_rmse

#Project test predictors using the PCA rotation from the training set
test_predictors <- tug_test_df[, !(names(tug_test_df) %in% c("TUG.Time", "Gait.Time"))]
test_pca_scores <- predict(results, newdata = test_predictors)[, 1:12]

#pcr_pred <- predict(pcregmod, newdata = data.frame(scr = test_pca_scores))
test_df <- data.frame(TUG.Time = tug_test_df$TUG.Time, test_pca_scores)
pcr_pred <- predict(pcregmod, newdata = test_df)


y_test <- test_df$TUG.Time
test_rmse <- sqrt(mean((pcr_pred - y_test)^2))
test_rmse
#Slightly different approach than data 311 when fitting PCR. Ran into some format issues with scr previously.
```

### Gait PLSR
```{r}
library(pls)

set.seed(30)
model <- plsr(Gait.Time ~ . - Gait.Time - TUG.Time, data = gait_train_df, scale=TRUE, method="oscorespls")
#Optimal is 1 components (Lowest RMSEP)
#Optimal is 7 components (Training/ 80% var explained)

pcr_pred <- predict(model, gait_test_df, ncomp=7)
y_test <- gait_test_df$Gait.Time
rmse_test <- sqrt(mean((pcr_pred - y_test)^2))

pcr_tr_pred <- predict(model, gait_train_df, ncomp=7)
y_pred <- gait_train_df$Gait.Time
rmse_train <- sqrt(mean((pcr_tr_pred - y_pred)^2))

rmse_train; rmse_test
```

###TUG PCR/PLSR
```{r}
#Keep number of components such that 80% of the variance from original data is retained.
#Scree plot says 3 components 
#Kaiser criterion says 23 for nondomiant, 40 for both
#13-14 if we want to retain 80% of variance (19 for both feet) Similar literature went this route

set.seed(30)
model <- plsr(TUG.Time ~ . - Gait.Time - TUG.Time, data = tug_train_df, scale=TRUE, method="oscorespls", validation="CV")
#Optimal is 2 components (Lowest RMSEP)
#Optimal is 5 components (Training/ 80% var explained)

pcr_pred <- predict(model, tug_test_df, ncomp=5)
y_test <- tug_test_df$TUG.Time
rmse_test <- sqrt(mean((pcr_pred - y_test)^2))

pcr_tr_pred <- predict(model, tug_train_df, ncomp=5)
y_pred <- tug_train_df$TUG.Time
rmse_train <- sqrt(mean((pcr_tr_pred - y_pred)^2))

rmse_train; rmse_test
```

## Elastic Net

### Gait Elastic
```{r}
#parameters
alphas <- seq(0, 1, 0.1)
n_iter <- 100

new_df <- ndom_df
names(new_df) <- make.names(names(new_df), unique = TRUE)
# build one model matrix from full df to capture correct predictor names
x_all <- model.matrix(Gait.Time ~ . - Gait.Time - TUG.Time, data=new_df)[,-1]   # drop intercept
colnames_x <- colnames(x_all)

#remove columns corresponding to response vars if they slipped in
colnames_x <- colnames_x[!colnames_x %in% c("Gait.Time", "TUG.Time")]

# storage for feature selection counts
feature_counts <- matrix(0, nrow = length(colnames_x), ncol = length(alphas))
colnames(feature_counts) <- paste0("alpha_", alphas)
rownames(feature_counts) <- colnames_x

mse_results  <- matrix(NA, nrow = n_iter, ncol = length(alphas))
rmse_results <- matrix(NA, nrow = n_iter, ncol = length(alphas))
rsq_results  <- matrix(NA, nrow = n_iter, ncol = length(alphas))

colnames(mse_results)  <- paste0("alpha_", alphas)
colnames(rmse_results) <- paste0("alpha_", alphas)
colnames(rsq_results)  <- paste0("alpha_", alphas)

for (i in 1:n_iter) {
  
  # random 70/30 split
  gait_train_index <- createDataPartition(new_df$Gait.Time, p = 0.7, list = FALSE)
  gait_train_df <- new_df[gait_train_index, ]
  gait_test_df  <- new_df[-gait_train_index, ]
  
  #names(gait_train_df) <- make.names(names(gait_train_df), unique = TRUE)
  #names(gait_test_df) <- make.names(names(gait_test_df), unique = TRUE)
  
  y_train <- gait_train_df$Gait.Time
  x_train <- model.matrix(Gait.Time ~ . - Gait.Time - TUG.Time, data = gait_train_df)
  
  y_test <- gait_test_df$Gait.Time
  x_test <- model.matrix(Gait.Time ~ . - Gait.Time - TUG.Time, data = gait_test_df)
  
  for (a in seq_along(alphas)) {
    alpha_val <- alphas[a]
    
    # cross-validation for best lambda
    cv_fit <- cv.glmnet(x_train, y_train, alpha = alpha_val, nfolds = 10)
    best_lambda <- cv_fit$lambda.min
    
    # fit model
    final_fit <- glmnet(x_train, y_train, alpha = alpha_val, lambda = best_lambda)
    
    # prediction
    y_pred <- predict(final_fit, s = best_lambda, newx = x_test)
    
    # metrics
    mse <- mean((y_test - y_pred)^2)
    rmse <- sqrt(mse)
    rsq <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)
    
    mse_results[i, a]  <- mse
    rmse_results[i, a] <- rmse
    rsq_results[i, a]  <- rsq
    
    # track selected features (nonzero coefficients)
    coefs <- coef(final_fit)
    selected <- rownames(coefs)[which(coefs != 0)]
    selected <- selected[selected != "(Intercept)"]
    feature_counts[selected, a] <- feature_counts[selected, a] + 1
  }
}

# aggregate results across iterations
avg_mse  <- colMeans(mse_results,  na.rm = TRUE)
avg_rmse <- colMeans(rmse_results, na.rm = TRUE)
avg_rsq  <- colMeans(rsq_results,  na.rm = TRUE)

best_alpha <- alphas[which.min(avg_mse)]

cat("Best alpha:", best_alpha, "\n\n")
cat("Average MSE:\n");  print(avg_mse)
cat("\nAverage RMSE:\n"); print(avg_rmse)
cat("\nAverage R-squared:\n"); print(avg_rsq)

# top 10 most frequently selected features at best alpha
selected_freq <- feature_counts[, paste0("alpha_", best_alpha)]
top_features <- sort(selected_freq, decreasing = TRUE)[1:10]

cat("\nTop 10 selected features:\n")
print(top_features)
#Best Alpha 0

#AI assisted with format and errors while creating this nested for loop.
```

```{r}
set.seed(101)
gait_train_index <- createDataPartition(new_df$Gait.Time, p = 0.7, list = FALSE)
gait_train_df <- new_df[gait_train_index, ]
gait_test_df  <- new_df[-gait_train_index, ]

y <- gait_train_df$Gait.Time

x <- model.matrix(Gait.Time ~ . - Gait.Time - TUG.Time, data=gait_train_df)

whole_lasso_fit <- glmnet(x,y,alpha=0.1)

#plot(whole_lasso_fit, xvar="lambda")
set.seed(101)
whole_cv_lasso <- cv.glmnet(x, y, alpha =0.1)

#plot(whole_cv_lasso)

#best lambdas
best_lambda <- whole_cv_lasso$lambda.min  #lambda that minimizes CV error/test MSE
best_lambda_1se <- whole_cv_lasso$lambda.1se  #1-SE rule (simpler model)
#best_lambda
#best_lambda_1se

#coef(whole_cv_lasso, s = "lambda.min") #coefficients at best lambda

lasmin <- glmnet(x,y,alpha = 0.1, lambda=best_lambda)

#coef(lasmin)
Gait_ela_columns <- rownames(coef(lasmin))[which(coef(lasmin) != 0)]
Gait_ela_columns <- Gait_ela_columns[Gait_ela_columns != "(Intercept)"]

x_test <- model.matrix(Gait.Time ~ . - Gait.Time - TUG.Time, data = gait_test_df)
y_test <- gait_test_df$Gait.Time

y_pred <- predict(lasmin, s = best_lambda, newx = x_test)

rmse_test <- sqrt(mean((y_test - y_pred)^2))
rmse_test

y_train_pred <- predict(lasmin, s = best_lambda, newx = x)

rmse_train <- sqrt(mean((y - y_train_pred)^2))
sst_train <- sum((y - mean(y))^2)
sse_train <- sum((y - y_train_pred)^2)
rsq_train <- 1 - sse_train/sst_train

rsq_train
rmse_train
```

### Tug Elastic
```{r}
#parameters
alphas <- seq(0, 1, 0.1)
n_iter <- 100

# build one model matrix from full df to capture correct predictor names
x_all <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time, data=new_df)[,-1]   # drop intercept
#x_all <- model.matrix(`TUG Time` ~ ., data=df)  
colnames_x <- colnames(x_all)

# remove dummy columns corresponding to response vars if they slipped in
colnames_x <- colnames_x[!colnames_x %in% c("Gait.Time", "TUG.Time")]

# storage for feature selection counts
feature_counts <- matrix(0, nrow = length(colnames_x), ncol = length(alphas))
colnames(feature_counts) <- paste0("alpha_", alphas)
rownames(feature_counts) <- colnames_x

mse_results  <- matrix(NA, nrow = n_iter, ncol = length(alphas))
rmse_results <- matrix(NA, nrow = n_iter, ncol = length(alphas))
rsq_results  <- matrix(NA, nrow = n_iter, ncol = length(alphas))

colnames(mse_results)  <- paste0("alpha_", alphas)
colnames(rmse_results) <- paste0("alpha_", alphas)
colnames(rsq_results)  <- paste0("alpha_", alphas)

for (i in 1:n_iter) {
  
  # random 70/30 split
  tug_train_index <- createDataPartition(new_df$TUG.Time, p = 0.7, list = FALSE)
  tug_train_df <- new_df[tug_train_index, ]
  tug_test_df  <- new_df[-tug_train_index, ]
  
  y_train <- tug_train_df$TUG.Time
  x_train <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time, data = tug_train_df)
  
  y_test <- tug_test_df$TUG.Time
  x_test <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time, data = tug_test_df)
  
  for (a in seq_along(alphas)) {
    alpha_val <- alphas[a]
    
    # cross-validation for best lambda
    cv_fit <- cv.glmnet(x_train, y_train, alpha = alpha_val, nfolds = 10)
    best_lambda <- cv_fit$lambda.min
    
    # fit model
    final_fit <- glmnet(x_train, y_train, alpha = alpha_val, lambda = best_lambda)
    
    # prediction
    y_pred <- predict(final_fit, s = best_lambda, newx = x_test)
    
    # metrics
    mse <- mean((y_test - y_pred)^2)
    rmse <- sqrt(mse)
    rsq <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)
    
    mse_results[i, a]  <- mse
    rmse_results[i, a] <- rmse
    rsq_results[i, a]  <- rsq
    
    # track selected features (nonzero coefficients)
    coefs <- coef(final_fit)
    selected <- rownames(coefs)[which(coefs != 0)]
    selected <- selected[selected != "(Intercept)"]
    feature_counts[selected, a] <- feature_counts[selected, a] + 1
  }
}

# aggregate results across iterations
avg_mse  <- colMeans(mse_results,  na.rm = TRUE)
avg_rmse <- colMeans(rmse_results, na.rm = TRUE)
avg_rsq  <- colMeans(rsq_results,  na.rm = TRUE)

best_alpha <- alphas[which.min(avg_mse)]

cat("Best alpha:", best_alpha, "\n\n")
cat("Average MSE:\n");  print(avg_mse)
cat("\nAverage RMSE:\n"); print(avg_rmse)
cat("\nAverage R-squared:\n"); print(avg_rsq)

# top 10 most frequently selected features at best alpha
selected_freq <- feature_counts[, paste0("alpha_", best_alpha)]
top_features <- sort(selected_freq, decreasing = TRUE)[1:10]

cat("\nTop 10 selected features:\n")
print(top_features)
#Best Alpha 0
```

```{r}
set.seed(101)
tug_train_index <- createDataPartition(new_df$TUG.Time, p = 0.7, list = FALSE)
tug_train_df <- new_df[tug_train_index, ]
tug_test_df  <- new_df[-tug_train_index, ]

y <- tug_train_df$TUG.Time

x <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time, data=tug_train_df)

whole_lasso_fit <- glmnet(x,y,alpha=0.1)

#plot(whole_lasso_fit, xvar="lambda")

set.seed(101)
whole_cv_lasso <- cv.glmnet(x, y, alpha = 0.1)

#plot(whole_cv_lasso)

#best lambdas
best_lambda <- whole_cv_lasso$lambda.min       #λ that minimizes CV error
best_lambda_1se <- whole_cv_lasso$lambda.1se   #1-SE rule (simpler model)
#best_lambda
#best_lambda_1se

#coef(whole_cv_lasso, s = "lambda.min")   #coefficients at best λ
lasmin <- glmnet(x,y,alpha = 0.1, lambda=best_lambda)

#tug_coef <- coef(lasmin)
#coef(lasmin)
Tug_ela_columns <- rownames(coef(lasmin))[which(coef(lasmin) != 0)]
Tug_ela_columns <- Tug_ela_columns[Tug_ela_columns != "(Intercept)"]

x_test <- model.matrix(TUG.Time ~ . - Gait.Time - TUG.Time, data = tug_test_df)
y_test <- tug_test_df$TUG.Time

y_pred <- predict(lasmin, s = best_lambda, newx = x_test)

rmse_test <- sqrt(mean((y_test - y_pred)^2))
rmse_test

y_train_pred <- predict(lasmin, s = best_lambda, newx = x)

rmse_train <- sqrt(mean((y - y_train_pred)^2))
sst_train <- sum((y - mean(y))^2)
sse_train <- sum((y - y_train_pred)^2)
rsq_train <- 1 - sse_train/sst_train

rmse_train
rsq_train
```

## SHAP

### Random Forest (Gait)
```{r}
#library(iml)
#library(shapper)
#library(shapleyR)

library(shapviz)

X_train <- subset(gait_train_df, select = -c(Gait.Time, TUG.Time))

sv <- shapviz(gaitRF, X = X_train)

#Global feature importance via mean(|SHAP|)
sv_importance(sv)

#Does not work since shapviz does not support randomForest package
```

```{r, cache=TRUE}
#Requires fastshap package to compute with randomForest() model
#Fastshap has global importance built in, therefore outputting mean absolute SHAP rankings (similar to literature)
#https://cran.r-project.org/web/packages/fastshap/fastshap.pdf
#https://bgreenwell.github.io/fastshap/index.html
library(shapviz)
library(fastshap)

X_train <- as.data.frame(subset(gait_train_df, select = -c(Gait.Time, TUG.Time)))

pred_fun <- function(object, newdata) {
  predict(object, newdata = as.data.frame(newdata))
}

#Compute approximate SHAP values
shap_values <- fastshap::explain(
  object = gaitRF,
  feature_names = colnames(X_train),
  newdata = X_train,
  pred_wrapper = pred_fun,
  X = X_train,     
  nsim = 100  #Can increase to 200–500 for smoother results
)
#AI assisted with "shap_values" data

sv <- shapviz(shap_values, X = X_train)

#Mean absolute SHAP rankings
sv_importance(sv)

#SHAP is quite computationally expensive.
```

## Bland-Altman (Gait)
```{r}
#https://www.statology.org/bland-altman-plot-r/ 
library(ggplot2)
#Lasso Fitted
observed <- gait_test_df$Gait.Time
predicted <- predict(gait_lasreg, newdata = gait_test_df)

BAdf <- data.frame(
  avg = (observed + predicted) / 2,
  diff = observed - predicted
)

colnames(BAdf) <- c("avg", "diff")

#Mean bias & limits of agreement
mean_diff <- mean(BAdf$diff)
sd_diff <- sd(BAdf$diff)
lower <- mean_diff - 1.96 * sd_diff
upper <- mean_diff + 1.96 * sd_diff

ggplot(BAdf, aes(x = avg, y = diff)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_hline(yintercept = mean_diff, color = "blue", linewidth = 1) +
  geom_hline(yintercept = lower, color = "red", linetype = "dashed") +
  geom_hline(yintercept = upper, color = "red", linetype = "dashed") +
  labs(
    title = "Bland–Altman Plot: Observed vs Predicted Gait Time",
    x = "Average of Observed and Predicted",
    y = "Observed - Predicted"
  ) +
  theme_minimal(base_size = 14)
```

## Bland-Altman (TUG)
```{r}
#Random forest
observed <- tug_test_df$TUG.Time #Also y_test
predicted <- predict(tugRF, newdata = tug_test_df)

BAdf <- data.frame(
  avg = (observed + predicted) / 2,
  diff = observed - predicted
)

colnames(BAdf) <- c("avg", "diff")

mean_diff <- mean(BAdf$diff)
sd_diff <- sd(BAdf$diff)
lower <- mean_diff - 1.96 * sd_diff
upper <- mean_diff + 1.96 * sd_diff

ggplot(BAdf, aes(x = avg, y = diff)) +
  geom_point(size = 2, alpha = 0.7) +
  geom_hline(yintercept = mean_diff, color = "blue", linewidth = 1) +
  geom_hline(yintercept = lower, color = "red", linetype = "dashed") +
  geom_hline(yintercept = upper, color = "red", linetype = "dashed") +
  labs(
    title = "Bland–Altman Plot: Observed vs Predicted TUG Time",
    x = "Average of Observed and Predicted",
    y = "Observed - Predicted"
  ) +
  theme_minimal(base_size = 14)
```
